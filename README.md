# Qualitative Analysis with Annotation Management
I lead a team of undergraduate research assistants in the annotation of a large corpus of data. 

This included collaborating on a _'codebook'_ that served as a term of reference for the team. I developed a protocol to test our annotation application that involved mathematical measurement of agreement according to standards of the research field.

## Methodology
We began with a sample reading of the same 10 documents to establish our own personal annotations, relative to the project's research questions, and that were based on our different knowledge of the field (e.g. experiential, personal, professional, or academic knowledge). We convened to share initial annotations and generate codes that would govern our subsequent readings. We convened again after 10 more documents and re-iterated the codebook.

Codebook entries eventually looked like this:
> # Role> Help Dynamic > Intermediary: Funder/Granting Agency
> ## Description
> This intermediary is implicated in the provision or supply of resources _for the research_. The status of help as a research condition or stipulation (e.g. being met through a grant proposal) is unknown.
>## Keywords/’Find’ Logic
> * Support, Grant/Fund/Award/Scholarship/Fellowship - associated bodies, like NSF, NSERC, etc.
> * Thanks or Involvement - of an acknowledged business or organization
> * Often found outside the main body of the publication such as in acknowledgements or disclosure sections 
>## Excerpts
> _“the Canada Foundation for Innovation Infrastructure Fund 33151 “Facility for Fully Interactive Physio-digital Spaces”_ (Roy 2021)
> 
> _“This research has been recognized and endorsed by the Office of the Research Ethics Commission in the people of Mahidol University. COA No. MU-CIRB 2018/018.1901.”_ (Pruettikomon 2018)
> ## Justification
> Help can be an implicitly meaningful mandate or action-premise. Help can also be instrumentally enrolled for its social-positivity. It is thus important to parse the economic drivers that may enroll help-premises and inform the normate around disabilityof in assistive technology research. 

After the convening period, the team coded the rest of the corpus documents and re-coded the first 20 documents according to the reference codebook. We used a qualitative analysis software for this.

I then developed a coding test to see how we individually met the codebook, in terms of agreement and overlap of the underlying data. Performance was measured by Cohen's Kappa coefficient - which is a well-established measure of inter-coder reliability in many research fields. Our coefficient was _'good'_ (~ 0.7) relative to other work in our field. 

With one more round of iteration, we achieved _'very good'_ (0.81).  

## What I Learned
I learned that collaborative work involving annotation can go beyond arguments of subjective interpretation and can be quantitatively supported. Especially where few ideas are rejected early on, an iterative process of annotation or coding that is supported by mathematical tests can ensure that the best ideas are retained and refined. It also helps to cohere the team, in the sense that all ideas are seen as valued and tested.

## Skills Used
* Qualitative Methods
* Quantitative Methods
* Bibliographic Management

## Tools Used
* Excel, Tableau
* Dedoose
* Qualcoder
* Zotero

