# Qualitative Analysis with Annotation Management
I lead a team of undergraduate research assistants in the annotation of a large corpus of data. 

This included collaborating on a _'codebook'_ that served as a term of reference for the team. I developed a protocol to test our annotation application that involved mathematical measurement of agreement according to standards of the research field.

## Methodology
We began with a sample reading of the same 10 documents to establish our own personal annotations, relative to the project's research questions, and that were based on our different knowledge of the field (e.g. experiential, personal, professional, or academic knowledge). We convened to share initial annotations and generate codes that would govern our subsequent readings. We convened again after 10 more documents and re-iterated the codebook.

After the convening period, we each coded the rest of the corpus documents and the first 20 documents according to the reference codebook. We used a qualitative analysis software for this.

I then developed a coding test to see how we individually met the codebook, in terms of agreement and overlap of the underlying data. Performance was measured by Cohen's Kappa coefficient - which is a well-established measure of inter-coder reliability in many research fields. Our coefficient was 'good' (~ 0.7) relative to other work in our field and with one more round of iteration, we achieved 'very good' (0.81).  

## What I Learned
I learned that collaborative work can be quantitatively supported. Especially where few ideas are rejected early on, an iterative process of annotation or coding that is supported by mathematical tests can ensure that the best ideas are retained and refined.

## Skills Used
* Qualitative Methods
* Quantitative Methods
* Bibliographic Management

## Tools Used
* Excel, Tableau
* Dedoose
* Qualcoder
* Zotero

